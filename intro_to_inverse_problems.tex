\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Img}{\mathrm{Img}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading


\begin{document}

\begin{center}
\Large
A Brief Introduction to Linear Inverse Problems in Hilbert Space
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

The study of inverse problems is generally concerned with approximating 
an unknown value using potentially corrupted or noisy data. The study of 
such problems finds many applications across the physical, social, and 
mathematical sciences. 

This document is meant to help you prepare an Article for
submission to \textsc{Mathematics Magazine}.  Of course,
editorial decisions depend entirely on
what you say and how you say it. Nonetheless, we will all save
time if you exercise some care in how you first present the paper.

Now that I have caught your attention with an interesting introductory paragraph,
here is what you will find:
specific information about the style of Articles in the \textsc{Magazine}
and a description of the \LaTeX\ code we prefer that you use
to prepare your manuscript.

Since this section is very clearly
an introduction, I thought that labeling it ``Introduction" would add
nothing.  Note that I am willing to use the first person in an Article
and you might be as well.  Another equally respectable choice is ``we,''
even when there is only one author; this can
create an author-reader partnership to
work through the mathematics together.
Whatever voice you choose, consistency is important.


\subsection*{Introduction to Inverse Problems} 
Students of linear algebra may recall the prominence of the matrix equation
\begin{equation} 
Ax = b. \label{matrix_eqn}
\end{equation}  
Here we may consider $A$ as a linear map 
that acts on a given vector $x$ and produces the output vector $b$. 
This constitutes the ``forward problem'' in which we are given inputs and must 
calculate outputs. The corresponding ``inverse problem'' reverses the situation; we observe 
$b$ and must determine the input $x$ which produced it. In its simplest form we may assume 
$A$ is invertible, in which case the solution to the inverse problem is given by matrix 
inversion: $x = A^{-1}b$. This paper introduces a vast generalization of this inverse 
problem framework, shedding the simplifying assumptions of finite dimensionality
and invertibility. The motivation for such a general theory 
of inverse problems will become clear with examples, as the majority of interesting 
applications fail to enjoy the ``well-posedness'' of the above equation. 
To make these notions concrete, consider the following mapping:
\begin{eqnarray} 
&F: X \to Y \\
&F(x) = y \label{inverse_problem}
\end{eqnarray} 
This equation, which generalizes the above matrix equation, will constitute the primary subject 
of our analysis. The goal remains to compute $x$ given $y$. Note that I have been intentionally 
vague regarding specific properties of the map $F$ or spaces $X$ and $Y$. Indeed, varying the assumptions 
placed on these objects lead to drastically different avenues of analysis and conceptual interpretations
of inverse problems. 

\paragraph{Ill-Posedness}

If all real-world examples of inverse problems admitted straightforward representations as in \ref{matrix_eqn}
then the field of inverse problem theory would not exist. The essence of the field is in developing methods to 
deal with the difficult cases. The following definition highlights the central difficulties that can occur, 
and motivates much of the theory discussed in this paper. 





Before introducing the assumptions that will guide the remainder of this paper, I briefly
state two alternative viewpoints that will not be discussed here. 

\paragraph*{Statistical Estimation}

Introduce the concept of residuals and give the statistical interpretation. Then talk about regularization briefly
and Bayesian inverse problems. 

\paragraph*{Non-linear Problems}

Give some examples and talk briefly about the close connection with differential equations. 

It is important to note that these assumptions are not mutually exclusive; the realities of specific applications
may necessitate various sets of assumptions. 



\textbf{TODO}
\begin{enumerate} 
\item Brief history of field
\item Purpose/overview of paper
\item Organization of paper
\end{enumerate} 

\section{Inverse Problems}

\subsection{Introduction} \hfill \\
 

\subsection{Ill-Posed Problems} \hfill \\
The following definition highlights the central difficulties in studying inverse problems, and motivates much of the theory discussed in this paper. \\


\textbf{Definition} \textit{Hadamard's Conditions for Well-Posedness}: (EL) Problem (1.1) is \textit{well-posed} in the sense of Hadamard provided it meets the three following conditions: 
\begin{enumerate} 
\item (Existence) A solution always exists; that is, given any $y \in Y$ there exists an $x \in X$ such that $F(x) = y$. 
\item (Uniqueness) The solution is unique; that is, if $F(x_1) = F(x_2)$ then $x_1 = x_2$.
\item (Stability) The solution is a continuous function of the input data; that is, if $F(x_n) \to y$  then $x_n \to x$ \\
If any of these conditions are not met, the problem is called \textit{ill-posed}. 
\end{enumerate} 

The treatment of of ill-posedness is the primary focus in the study of inverse problems, in particular the development of techniques to handle violations of the third condition. Violations of the first two conditions are typically handled by instead seeking the ``closest'' solution in some sense; that is, by reformulating the problem as a minimum norm problem: 

\begin{align*}
&\argmin_{x \in X} \norm{F(x) - y}
\end{align*}
 
 The violation of stability presents a more difficult challenge. In general we imagine $y$ to be data measured imprecisely. Therefore, the concern is that small fluctuations in the observations $y$ may lead to large deviations in $x$. Regularization methods form the foundation of the toolkit used to address this problem and will be discussed later in this paper. 
 
 \subsection{Theoretical Framework} \hfill \\
 
 \subsection{Examples} \hfill \\
 An abstract, general formulation of inverse problems may not initially appear useful. However, it soon becomes clear that the generality allows these techniques to be applied to a wide array of problems across various fields. Many of these applications are naturally formulated in infinite-dimensional vector spaces, and moreover suffer from ill-posedness. I provide three examples below of such applications. 
 
 \subsubsection{Statistical Estimation} \hfill \\
 Parameter estimation (or \textit{model calibration}), ubiquitous across the fields of statistics and machine learning, may be formulated as an inverse problem. In this case we view $F_\theta$ as a model or process that depends on some parameters $\theta$. We may alternatively view $\theta$ as an additional input to the system, in which case $F$ acts on both the parameters and input data to produce the observed outputs; that is, $F(\theta, x) = y$. The problem thus becomes: given observed outputs $y$ and inputs $x$, estimate the parameters $\theta$ of the underlying process $F$. 
 
 \section{Preliminaries: Operator Theory and Probability}
 This paper assumes basic knowledge of the major results in the study of Banach and Hilbert Spaces, including the basic theory of linear maps between such spaces. However, 
 
 
 However, no background in operator theory is assumed. Given how intertwined the study of inverse problems is with the study of operators, all of the needed results are detailed here. Proofs are provided when deemed useful, with some being relegated to the appendix. 
 
 \subsection{Operator Theory} \hfill \\
 Throughout this paper, all vector spaces are assumed to be real. 
 
 \textbf{Definition}: A \textit{map} $F: X \to Y$ is function mapping between two vector spaces $X$ and $Y$, called the \textit{domain} and \textit{codomain}, respectively. If $F$ maps $x \in X$ to $y \in Y$ I write $F(x) = y$. I denote by $\Img F$ the set of all $y \in Y$ such that there is an $x \in X$ with $F(x) = y$. I denote by $\Ker F$ the set of all $x \in X$ that map to the zero vector $0 \in Y$. Letting $U \subset Y$, I define the \textit{inverse image} (or \textit{pre-image}) $F^{-1}(V)$ to be the set of all $x \in X$ satisfying $F(x) \in V$. 
 
 Throughout this paper, we restrict our attention to the study of linear inverse problems. Therefore, the following definition is central to the theory. 
 
 \textbf{Definition}: A \textit{linear map} (or \textit{operator}) is a map $F: X \to Y$ satisfying $F(\alpha_1x_1 + \alpha_2x_2) = \alpha_1F(x_1) + \alpha_2F(x_2)$ for all real numbers $\alpha_1, \alpha_2$ and vectors $x_1, x_2 \in X$. When a map is linear, I suppress the parentheses and write function evaluation as $Fx := F(x)$. 

In a manner similar to linear functional theory, it can be shown that the space of linear maps from $X$ to $Y$ is a vector space. Moreover, if $X$ and $Y$ are normed spaces we may define a norm on this space in similar fashion. 

\textbf{Definition}: Let $F$ be a linear map between normed linear spaces $X$ and $Y$. Define the norm of $F$ $\norm{F} := \sup_{x \neq \theta} \frac{\norm{Fx}}{x}$. 

It can, of course, be shown that this does indeed define a norm on this space. 


\section{Linear Inverse Problems in Hilbert Space}

Hilbert space provides a natural setting in which to study inverse problems. From a theoretical point of view, the rich theory of Hilbert space leads to relatively simple characterizations of solutions, while practically, many real-world inverse problems may naturally be formulated in such settings. \textbf{Insert example here that uses sum of squares norm}. Throughout this section, $X$ and $Y$ are assumed to be real Hilbert spaces with respective inner products $(\cdot | \cdot)_X$ and $(\cdot | \cdot)_Y$. Recall from section \textbf{2} that the natural approach to address the non-existence and non-uniqueness of $Tx = y$ was to consider the least-squares problem $\inf_{x \in X} \norm{Tx - y}_Y$. If we assume that $\Img T \subset Y$ is closed, this guarantees the existence of a unique $\hat{y} \in Y$ minimizing $\norm{y^\prime - y}_Y$ on $\Img T$. However, we have not assumed that $T$ is injective and hence this does not imply the uniqueness of a least-squares solution. Therefore, we must study the set of all least-squares solutions, with an emphasis on the solution of minimal norm. 

\subsection{Characterization of Solutions}

 \textbf{Definition} (generalized solutions): 
 \begin{itemize}
 \item $x \in X$ is called a \textit{least-squares solution} of 1.1 if $\norm{Tx - y}_Y \leq \norm{Tu - y}_Y$ for all $u \in X$. $\mathbb{L}$ denotes the set of all such solutions. 
 \item $x^\dagger \in X$ is called a \textit{minimal norm least-squares solution} if $x^\dagger \in \mathbb{L}$ and $\norm{x^\dagger}_X \leq \norm{x}_X$ for all $x \in \mathbb{L}$. 
 \end{itemize} 
 
 Note that in general $\mathbb{L}$ may be empty. The following assumption relies on the assumption that $\Img T$ is closed to ensure the existence of a solution. 
 
 \textbf{Proposition}: Suppose $\Img T$ is closed. Then there is a unique minimal norm least-squares solution $x^\dagger$. Moreover, $x^\dagger \in (\Ker T)^\perp$. \\[.1cm]
 
 \textit{Proof}: Since $\Img T$ is closed, then there exists a unique $\hat{y} \in Y$ satisfying $\hat{y} = \min_{y^\prime \in \Img T} \norm{y^\prime - y}_{Y}$. Let $\hat{x} \in \mathbb{L}$ such that $T\hat{x} = \hat{y}$. Then, 
 \begin{align*} 
 \mathbb{L} &= \{x \in X: Tx = \hat{y}\} \\
                   &= \{x \in X: T(x - \hat{x}) = 0\} \\
                   &= \hat{x} + \{x - \hat{x} \in X: T(x - \hat{x}) = 0\} \\
                   &= \hat{x} + \Ker T
 \end{align*}  
 
 Therefore, $ \mathbb{L}$ is a translation of the closed subspace $\Ker T$. It then follows immediately from Luenberger 3.10 Theorem 3 that there exists a unique $x^\dagger \in \mathbb{L}$ of minimal norm, and moreover that $x^\dagger$ is orthogonal to $\Ker T$. $\qquad \blacksquare$
 
 While the above proof is illustrative, the assumption that $\Img T$ is closed is too restricting in some applications (\textbf{add example}). Fortunately, loosening this restriction does not forfeit the uniqueness of the minimal norm solution. However, it does require the additional assumption that a least-squares solution exists at all. 
 
 \textbf{Proposition}: Suppose $\mathbb{L}$ is non-empty. Then the results of the above proposition still hold. 
 \textbf{NEED TO VERIFY THIS IS ACTUALLY TRUE: SEE EL THM 2.2; I believe it is true and I can use Thm 2.2 and the following corollary to prove it}
 \textit{Proof}: (EL) Let $x^\dagger$ be a minimal norm solution and suppose for a contradiction that $x^\dagger \notin \Ker T$. Since $\Ker T$ is a closed subspace then $X = \Ker T \oplus (\Ker T)^\perp$. Thus, there exists unique vectors $u \in \Ker T$ and $u^\perp \in (\Ker T)^\perp$ satisfying $x^\dagger = u + u^\perp$. Since $\norm{Tu^\perp - y}_X = \norm{T(x - u) - y}_X = \norm{Tx - y}_X$, it follows that $u^\perp \in \mathbb{L}$. Then, \[\norm{x^\perp}_{X}^{2} = \norm{u + u^\perp}_{X}^{2} = \norm{u}_{X}^{2} + \norm{u^\perp}_{X}^{2} > \norm{u^\perp}_{X}^{2}\]
 which contradicts $x^\dagger$ being a minimal norm solution. 
 
 \smallskip 
 
\textbf{Must show existence} 

\medskip

The following theorem generalizes the well-known normal equations to infinite-dimensional spaces. 

\textbf{Theorem}: $\hat{x} \in \mathbb{L}$ if and only if $T^* T\hat{x} = T^* y$. In this case $T\hat{x} - y \in (\Img T)^\perp$. \\[.1cm] 

\textit{Proof}: (L and S) First note that $\inf_{x \in X} \norm{Tx - y}_Y = \inf_{y^\prime \in \Img T} \norm{y^\prime - y}_Y$. By the projection theorem (Luenberger 3.3 Theorem 1) we have \[\hat{y} = \inf_{y^\prime \in \Img T} \norm{y^\prime - y}_Y \iff \hat{y} - y \perp \Img T\] 
The orthogonality condition follows from the fact that $\hat{x} \in \mathbb{L}$ if and only if $T\hat{x} = \hat{y}$. 

\smallskip

To obtain the normal equations, consider: 
\begin{align*} 
\hat{x} = \inf_{x \in X} \norm{Tx - y}_X &\iff (T\hat{x} - y) \perp \Img T \\
                                                            &\iff (T\hat{x} - y| Tx) = 0 \ \forall x \in X \\
                                                            &\iff (T^*(T\hat{x} - y)| x) = 0 \ \forall x \in X \\
                                                            &\iff (T^*T\hat{x} - T^*y| x) = 0 \ \forall x \in X \\
                                                            &\iff T^*T\hat{x} - T^*y = 0 \\
                                                            &\iff T^*T\hat{x} = T^*y                                                            
\end{align*} 
where the first step uses the above portion of the proof and the remainder utilize the basic properties of the adjoint and inner product. $\qquad \blacksquare$
 
 \subsection{Finding Minimal Norm Solutions} \hfill \\
 
 In the above section, we verified (under suitable assumptions) the existence of a unique minimal norm solution. We now turn our attention to developing techniques to find this solution. The standard tool in this effort is the \textit{Moore-Penrose pseudoinverse}. As we are not guaranteed a unique least-squares solution, the standard concept of an ``inverse'' does not apply. However, we now observe the utility of having proved results that guarantee a unique minimal norm solution, which allows us to generalize the inverse as an operator that maps to such solutions. 
 
 \textbf{Definition}: For $T \in B(X, Y)$, define $\tilde{T}$ to be the restriction of $T$ to $(\Ker T)^\perp$. The \textit{Moore-Penrose psuedoinverse} $T^\dagger : (\Img T) \oplus (\Img T)^\perp \to Y$ is then defined as follows: 
 \begin{itemize} 
 \item For all $y \in \Img T$, $T^\dagger y := T^{-1}y$
 \item $T^\dagger$ is extended to $(\Img T) \oplus (\Img T)^\perp$ by requiring that $T^\dagger$ is linear and satisfies $T^\dagger y^\perp = 0$ for all $y^\perp \in (\Img T)^\perp$
 \end{itemize} 
 
Note that by definition $\tilde{T}$ has a trivial null space and we are therefore justified in talking about its inverse. Recall from \textbf{prop num} that the requirement $y \in (\Img T) \oplus (\Img T)^\perp$ guaranteed uniqueness of the minimal norm solution. It is therefore unsurprising that we define the domain of $T^\dagger$ to be $(\Img T) \oplus (\Img T)^\perp$. In particular, if $\Img T$ is closed then $T^\dagger$ is defined on all of $Y$. While the above definition may seem cryptic, the next result shows that it addresses our problem of finding minimal norm solutions.

\textbf{Proposition}: Let $y \in (\Img T) \oplus (\Img T)^\perp$. Then $T^\dagger y$ is the minimal norm solution of the least-squares problem. 

\smallskip 

\textit{Proof}: By \textbf{prop num}, a unique minimal norm solution exists. There are unique $\hat{y} \in \Img T$ and $y^\perp \in (\Img T)^\perp$ such that $y = \hat{y} + y^\perp$. Since $y - \hat{y} \perp \Img T$ it follows from the projection theorem (Luenberger 3.3 Theorem 1) that $\hat{y} = \argmin_{y^\prime \in \Img T} \norm{y^\prime - y}_Y$. By definition of $T^\dagger$, we have \[T^\dagger y = T^\dagger \hat{y} = \tilde{T}^{-1}\hat{y}\]
 Applying $T$ to both sides, \[T(T^\dagger y) = \hat{y}\]
 Thus $T^\dagger y \in \mathbb{L}$. By definition $T^\dagger y \in (\Ker T)^\perp$ so $T^\dagger y$ is the minimal norm solution (\textbf{prop num}). $\qquad \blacksquare$
 
 
 
 
 
 
 

\end{document} 