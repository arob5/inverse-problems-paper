\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

\begin{document}

\begin{center}
\Large
A Brief Introduction to Linear Inverse Problems in Hilbert Space
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Introduction
The study of inverse problems is generally concerned with approximating 
an unknown value using potentially corrupted or noisy data. Such problems find
many applications across the physical, social, and mathematical sciences. Many 
classic examples of inverse problems stem from the earth sciences; indeed,
environmental measurements are often \textit{indirect}, meaning that we measure a quantity
that depends on some other input in the hope of producing an estimate of this 
underlying input. The goal of inverse problem theory is to develop methods to make stable
estimates of the underlying quantity in such scenarios. 

This goal of this paper is to provide a brief introduction to modeling a specific class of inverse
problems; namely, linear inverse problems in Hilbert space.
The focus here is on introducing some basic theoretical tools from Hilbert space and 
operator theory and demonstrating how they may be used to characterize the 
solutions of inverse problems. Techniques to actually solve such problems are beyond the
scope of this introduction. 

% Section 1: Introduction to Inverse Problems
\section{Introduction to Inverse Problems} \label{inverse_problem_intro}

\subsection{Motivation}
Students of linear algebra may recall the importance of the matrix equation
\begin{equation} 
Ax = b. \label{matrix_eqn}
\end{equation}  
Here we consider $A$ as a linear map 
that acts on a given vector $x$ and produces the output vector $b$. 
This constitutes the ``forward problem'' in which we are given inputs and must 
calculate outputs. The corresponding ``inverse problem'' reverses the situation; we observe 
$b$ and must determine the input $x$ which produced it. In its simplest form we may assume 
$A$ is invertible, in which case the solution to the inverse problem is given by matrix 
inversion: $x = A^{-1}b$. This paper introduces a vast generalization of this inverse 
problem framework, shedding the simplifying assumptions of finite dimensionality
and invertibility. The motivation for such a general theory 
of inverse problems will become clear with examples, as the majority of interesting 
applications fail to enjoy the ``well-posedness'' of the above equation. 
To make these notions concrete, consider the following mapping:
\begin{eqnarray} 
&F: X \to Y \nonumber \\
&F(x) = y \label{inverse_problem}
\end{eqnarray} 
This equation, which generalizes the above matrix equation, will constitute the primary subject 
of our analysis. The goal remains to compute $x$ given the measurement $y$. We have been intentionally 
vague regarding specific properties of the map $F$ or spaces $X$ and $Y$. Indeed, varying the assumptions 
placed on these objects lead to drastically different avenues of analysis and conceptual interpretations
of inverse problems. 

\subsection{Ill-Posedness}

Most real-world examples of inverse problems do not admit straightforward representations as in equation \eqref{matrix_eqn}.
The essence of studying such problems is in developing methods to 
deal with the so-called ``ill-posed'' cases. The following definition makes this notion concrete, 
and motivates much of the theory discussed in this paper. 

\begin{definition} 
(Hadamard's Conditions)The inverse problem given by equation \eqref{inverse_problem} is well-posed in the sense of Hadamard 
provided it satisfies the three following conditions:
\begin{enumerate} 
\item (Existence) A solution always exists; that is, given any $y \in Y$ there exists an $x \in X$ such that $F(x) = y$. 
\item (Uniqueness) The solution is unique; that is, if $F(x_1) = F(x_2)$ then $x_1 = x_2$.
\item (Stability) The solution is a continuous function of the input data; that is, if $F(x_n) \to y$  then $x_n \to x$
\end{enumerate} 
If any of these conditions are not met, the problem is called ill-posed. 
\end{definition} 

Given that the central task in studying inverse problems is to mitigate ill-posedness, it is worthwhile to make some comments here. 
Violations of the first two conditions are typically handled by 
instead seeking the ``closest'' solution in some sense; that is, by reformulating the problem as a minimum norm problem: 

\begin{equation}
\argmin_{x \in X} \norm{F(x) - y} \label{min_norm}
\end{equation}

 The violation of stability presents a more difficult challenge. In general we imagine $y$ to be data measured imprecisely. 
 Therefore, the concern is that small fluctuations in the observations $y$ may lead to large deviations in $x$. 
Our goal is to minimize the \textit{forward error} $\norm{x - x_0}$, where $x_0$ is some unknown true solution. However, 
given that we don't know $x_0$, we must instead focus our efforts on minimizing the \textit{backward error}, $\norm{F(x) - y}$. 
Stated in this terminology, the stability condition means that small backward errors imply small forward errors. 

 So-called \textit{regularization} methods form the foundation of the toolkit used to address instability, but will not be discussed 
 in this paper.  

\subsection{Theoretical Framework}

Hilbert space provides a natural setting in which to study inverse problems. From a theoretical point of view, the structure provided by 
inner products leads to relatively simple characterizations of solutions, while practically, many applications may 
naturally be formulated in such settings. Therefore, throughout this paper, we consider the inverse problem \eqref{inverse_problem}
where $X$ and $Y$ are assumed to be real Hilbert spaces with respective inner products $(\cdot | \cdot)_X$ and $(\cdot | \cdot)_Y$. 
Moreover, we assume $F$ to be a bounded linear operator between these spaces. 

It is important to note that while we have restricted ourselves to these assumptions, inverse problems can be formulated in many 
other settings. Perhaps the most notable omission here is any sort of statistical treatment. It is common to treat $X$ and $Y$ as
probability spaces and $x$ and $y$ as random variables. In this setting, the given ``data'' is the conditional random variable 
$y|x$, and solving the inverse problem means estimating the random variable $x|y$. While such a formulation is common, this 
paper will treat the solution of \eqref{inverse_problem} as purely deterministic.  

 \subsection{Examples}
 
 An abstract, general formulation of inverse problems may not initially appear useful. 
 However, it soon becomes clear that the generality allows these techniques to be 
 applied to a wide array of problems across various fields. Many of these applications 
 are naturally formulated in infinite-dimensional vector spaces, and moreover suffer from
  ill-posedness. I provide two examples below of such applications. 
 
 \paragraph*{Statistical Estimation}
 Parameter estimation (or \textit{model calibration}), ubiquitous across the fields of statistics 
 and machine learning, may be formulated as an inverse problem. In this case we view 
 $F_\theta$ as a model or process that depends on some parameters $\theta$. We may
  alternatively view $\theta$ as an additional input to the system, in which case $F$ acts 
  on both the parameters and input data to produce the observed outputs; that is, 
  $F(\theta, x) = y$. The problem thus becomes: given observed outputs $y$ and inputs $x$, 
  estimate the parameters $\theta$ of the underlying process $F$. 
  
  \paragraph*{Deconvolution}
 A common problem in signal and image processing, deconvolution describes the process of 
 ``deblurring'' a degraded signal or image. This may be modeled as 
 \[Fx = y\]
 where the map $F$ is defined by 
 \[y(\alpha) = (Fx)(\alpha) = \int_{-\infty}^{\infty} x(t)g(\alpha - t) dt\]
 We think of $y$ as an observed signal that has been distorted by some known function $g$ (often $g$ 
 is unknown and hence must be estimated first). The goal is then to recover the original unknown signal $x$.
 This is typically performed by applying the inverse Fourier transform; however, this inverse map often
 magnifies small measurement errors in $y$ and hence is ill-posed. Since $x$ and $y$ are functions, 
 we observe that this inverse problem is inherently infinite-dimensional. 

 
 % Section 2: Preliminaries
 \section{Preliminaries: Operator Theory} \label{preliminaries}
 This paper assumes prior knowledge of Hilbert spaces, including the basic theory of bounded 
 linear maps between such spaces. However, the adjoint map and projection operator, which are of central importance 
 to the study of inverse problems, are introduced from the ground up. As a notational aside, $B(X, Y)$ denotes the space of all bounded
 (or equivalently, continuous) linear operators from $X$ to $Y$. For $F \in B(X, Y)$ we write $\norm{F}$
 to refer to the operator norm of $F$. $\mathcal{R}(F)$ and $\mathcal{N}(F)$ denote the range and 
 null space of $F$, respectively. Finally, if a bounded linear map shares the same domain and codomain we write 
 $F \in B(X, X) = B(X)$ and call $F$ an \textit{operator} on $X$. 
 
\subsection{Adjoints}
 \begin{definition} 
 (Adjoint)\footnote{The adjoint may also be defined more generally for Banach spaces (Luenberger 1969, p. 150).} 
 	Let $F \in B(X, Y)$. Then the adjoint of $F$, $F^*: Y \to X$, is defined by 
 	\begin{equation*} 
	(Fx|y)_Y = (x|F^*y)_X
 	\end{equation*} 
	for all $x \in X$, $y \in Y$. 
 \end{definition} 
To justify this definition, consider the bounded linear functional $f_y(x) = (Fx|y)_Y$. By the Riesz-Frechet theorem (Luenberger 1969, p. 109)
there is a unique $x_y \in X$ satisfying $f_y(x) = (x|x_y)_X$ for all $x \in X$; that is, $(Fx|y)_Y = (x|x_y)_X$. 
We thus define $F^*y := x_y$. The adjoint helps to encode orthogonality relations between the spaces $X$ and $Y$. 
It may be interpreted as a generalization of the transpose operator for real-valued matrices; indeed, if $A \in \mathbb{R}^{n \times m}$, 
then $A^* = A^\prime$. The following three propositions outline the fundamental properties of adjoints that will be leveraged throughout
this paper. 

\begin{prop} 
Let $F \in B(X, Y)$. Then $F^* \in B(Y, X)$ and $\norm{F^*} = \norm{F}$. 
\end{prop}

\begin{proof} 
The linearity of $F^*$ follows directly from the definition of the adjoint and the linearity of the inner product. 
\begin{align*} 
(x|F^*(\alpha_1 y_1 + \alpha_2 y_2))_X &= (Fx|\alpha_1 y_1 + \alpha_2 y_2)_Y \\
                                                               &= \alpha_1(Fx|y_1)_Y + \alpha_2(Fx|y_2)_Y \\
                                                               &= \alpha_1(x|F^*y_1)_X + \alpha_2(x|F^*y_2)_X \\
                                                               &= (x|\alpha_1 F^*y_1 + \alpha_2 F^*y_2)_X
\end{align*} 
To show that $F^*$ is bounded, let $y \in Y$ be non-zero and apply the Cauchy-Schwarz inequality to obtain,
\begin{align*} 
\norm{F^*y}^2_X = (F^*y|F^*y)_X = (F(F^*y)|y)_Y &\leq \norm{F(F^*y)}_Y\norm{y}_Y \\
                                                                               &\leq \norm{F}\norm{F^*y}_X\norm{y}_Y
\end{align*}
Thus, $\norm{F^*y}_X \leq \norm{F}\norm{y}_Y$ for all non-zero $y \in Y$ and therefore $\norm{F^*} \leq \norm{F}$. 
This verifies $F^* \in B(Y, X)$. By similar reasoning, we obtain the reverse inequality. 
\begin{align*} 
\norm{Fx}^2_Y = (Fx|Fx)_Y = (x|F^*(Fx))_X &\leq \norm{F^*(Fx)}_X\norm{x}_X \\
                                                                      &\leq \norm{F^*}\norm{Fx}_Y\norm{x}_X
\end{align*}
Therefore, $\norm{Fx} \leq \norm{F^*}\norm{x}_X$ for all non-zero $x \in X$ and so $\norm{F} \leq \norm{F^*}$. 
Thus, $\norm{F} = \norm{F^*}$
\end{proof} 

The first two results in the following proposition tell us that the adjoint is a linear map from $B(X, Y)$
to $B(Y, X)$ (not to be confused with the fact that the adjoint of a fixed $F \in B(X, Y)$ is a linear map 
from $Y$ to $X$).

\begin{prop} 
Let $F, G \in B(X, Y)$ and $\alpha \in \mathbb{R}$. Then,
\begin{itemize}
\item $(F + G)^* = F^* + G^*$
\item $(\alpha F)^* = \alpha F^*$
\item $F^{**} = F$
\end{itemize}  
\end{prop}

\begin{proof} 
The first result follows from definition of the adjoint and linearity of the inner product. 
\begin{align*} 
(x|(F + G)^*y)_X &= ((F + G)x|y)_Y \\
                           &= (Fx|y)_Y + (Gx|y)_Y \\
                           &= (x|F^*y)_X + (x|G^*y)_X \\
                           &= (x|F^*y + G^*y)_X
\end{align*} 
Since $x$ and $y$ are arbitrary, this shows $(F + G)^* = F^* + G^*$. The second result follows from the 
same line of reasoning. For the third result, we know $F^{**}: X \to Y$ so 
\begin{equation*} 
(F^{**}x|y)_Y = (x|F^*y)_X = (Fx|y)_Y
\end{equation*} 
which shows $F^{**} = F$.
\end{proof} 

Recall from linear algebra that if $A \in \mathbb{R}^{n x m}$, then the row space of $A$ is orthogonal to 
the null space, and similarly the column space is orthogonal to the null space of $A^\prime$. The following
result generalizes this idea to Hilbert space, replacing the transpose with the adjoint. 

\begin{prop} \label{subspace_equalities}
Let $F \in B(X, Y)$. Then, 
\begin{itemize} 
\item $\mathcal{R}(F)^\perp = \mathcal{N}(F^*)$
\item $\overline{\mathcal{R}(F)} = \mathcal{N}(F^*)^\perp$
\item $\mathcal{R}(F^*)^\perp = \mathcal{N}(F)$
\item $\overline{\mathcal{R}(F^*)} = \mathcal{N}(F)^\perp$
\end{itemize} 
\end{prop} 

\begin{proof} 
Let $y^\prime \in \mathcal{N}(F^*)$ and $y \in \mathcal{R}(F)$. Then there exists $x \in X$
such that $Fx = y$. Then \[(Fx|y^\prime)_Y = (x|F^*y^\prime)_X = (x|0)_X = 0\]
so $\mathcal{N}(F^*) \subset \mathcal{R}(F)^\perp$. Now let $y^\perp \in \mathcal{R}(F)^\perp$. 
Then for any $x \in X$, \[(x|F^*y^\perp)_X = (Fx|y^\perp)_Y = 0\]
which implies $F^*y^\perp = 0$, or $y^\perp \in \mathcal{N}(F)$. 
Thus, $\mathcal{R}(F)^\perp = \mathcal{N}(F^*)$. Taking orthogonal complements, we obtain 
\[\mathcal{R}(F)^{\perp\perp} = \mathcal{N}(F^*)^\perp\]
or 
\[\overline{\mathcal{R}(F)} = \mathcal{N}(F^*)^\perp\]
The remaining two equalities follow directly from the fact that $F^{**} = F$. 
\end{proof} 


Note that if we assume the range of $F$ is closed then we may drop the closure in the second and third
results above. 

To conclude our introduction to the adjoint, we prove a result that more closely aligns with our goal of studying 
linear inverse problems. The restrictive assumption that the equation is consistent will be dropped in section
\ref{formulation}, where this result will be generalized to least-squares problems. 
\begin{prop} 
Let $F \in B(X, Y)$ with $\mathcal{R}(F) \subset Y$ closed. Assume there is at least one $x \in X$ satisfying
$Fx = y$. Then the vector $x^\dagger$ of minimum norm satisfying this equation is given by 
$x^\dagger = F^*y^\prime$, where $y^\prime$ is a solution of $FF^*y^\prime = y$. 
\end{prop} 

\begin{proof} 
Let $x \in X$ be any fixed solution to $Fx = y$. Then any vector of the form $x + k$, where $k \in \mathcal{N}(F)$, 
is also a solution. To show that this accounts for all possible solutions, let $x^\prime$ be another solution. 
This implies $F(x^\prime - x) = 0$ so $x^\prime - x \in \mathcal{N}(F)$ and thus $x^\prime$ must also be of the form 
$x + k$. Now that we have characterized the set of possible solutions, we must show how to select the solution
of minimal norm. The existence of such a unique solution follows from $\mathcal{N}(F)$ being a closed subspace, 
which implies the set of solutions to $Fx = y$ is the translation of a closed subspace. It then follows that the unique vector $x^\dagger$
of minimum norm is orthogonal to $\mathcal{N}(F)$ (Luenberger 1969, p. 64). From the previous proof and the assumption
that $F$ has closed range, \[x^\dagger \in \mathcal{N}(F)^\perp = \overline{\mathcal{R}(F^*)} = \mathcal{R}(F^*)\]
Therefore, there exists $y^\prime \in Y$ satisfying $F^*y^\prime = x^\dagger$. Since $x^\dagger$ is a solution of 
the original equation, we obtain $FF^*y^\prime = y$. 
\end{proof} 

\subsection{Projections}

Throughout this section we use the fact that if $M \subset H$ is a closed subspace of a Hilbert space, then 
$H$ may be written as the direct sum $H = M \oplus M^\perp$. For a given $x_i \in H$ we write $m_i \in M$ and 
$m_i^\perp \in M^\perp$ to denote the vectors giving the unique decomposition $x_i = m_i + m_i^\perp$. 

\begin{definition} 
(Projection Operator) Let $M$ be a closed subspace of a Hilbert space $H$. Then each $x \in H$ has a unique
representation $x = m + m^\perp$, where $m \in M$ and $m \in M^\perp$. The operator $P_M: H \to H$ defined 
by $P_Mx = m$ is called the projection operator onto $M$. 
\end{definition}

\begin{prop} \label{projection_properties}
If $M \subset H$ is a closed subspace, then: 
\begin{itemize} 
\item $P_M \in B(H)$
\item If $M \neq \{0\}$, then $\norm{P_M} = 1$
\item $I - P_M = P_{M^\perp}$
\end{itemize} 
\end{prop}

\begin{proof} 
For linearity, let $x_1 = m_1 + m_1^\perp$, $x_2 = m_2 + m_2^\perp \in H$ and $\alpha \in \mathbb{R}$. Then 
$x_1 + x_2 = (m_1 + m_2) + (m_1^\perp + m_2^\perp)$, where the first term is in 
$M$ and the second in $M^\perp$. Thus, \[P_M (x_1 + x_2) = m_1 + m_1 = P_M x_1 + P_M x_2\]
$P_M (\alpha x_1) = \alpha P_M x_1$ follows by the same reasoning, so $P_M$ is linear. 

To show that $P_M$ is bounded, we apply the generalized Pythagorean Theorem (Luenberger 1969, p. 49) to conclude: 
\[\norm{P_M x}_H^2 = \norm{m}_H^2 \leq \norm{m}_H^2 + \norm{m^\perp}_H^2 = \norm{x}_H^2\]
and therefore $\norm{P_M x}_H \leq \norm{x}_H$ for all $x \in H$ so $P_M$ is bounded. Thus, $P_M \in B(H)$
and $\norm{P_M} \leq 1$. 

For the reverse inequality, since $M \neq \{0\}$ we may select a vector $m \in M$ such that $\norm{m}_H = 1$.
Then $\norm{P_M m}_H = \norm{m}_H = 1$. This shows $\norm{P_M} \geq 1$. Therefore, $\norm{P_M} = 1$. 

The final result follows from, \[(I - P_M)(x) = (I - P_M)(m + m^\perp) = (m + m^\perp) - m = m^\perp = P_{M^\perp}x\]
Therefore, $I - P_M = P_{M^\perp}$. 
\end{proof} 

\begin{prop} 
An operator $P \in B(H)$ is a projection operator if and only if it satisfies the following two properties: 
\begin{itemize} 
\item $P$ is idempotent; that is, $P^2 = P$
\item $P$ is self-adjoint; that is, $P^* = P$
\end{itemize} 
\end{prop} 

\begin{proof} 
First suppose $P$ is a projection operator onto some closed subspace $M \subset H$. $P$ is clearly seen 
to be idempotent by applying the definition of a projection twice. To show that $P$ is self-adjoint, let $x_1, x_2 \in H$
and consider: 
\begin{align*} 
(Px_1|x_2)_H &= (m_1|m_2 + m_2^\perp)_H = (m_1|m_2)_H \\
(x_1|Px_2)_H &= (m_1 + m_1^\perp|m_2)_H = (m_2|m_2)_H
\end{align*} 
Therefore, $(Px_1|x_2)_H = (x_1|Px_2)_H$ for all $x_1, x_2 \in H$ so $P^* = P$. 

Now suppose $P$ is idempotent and self-adjoint. I claim that $P$ is a projection
upon $\R(P)$. Since $P^* = P$ it follows from proposition \eqref{subspace_equalities}
that $\overline{\R(P)} = \N(P)^\perp$. Let $M = \overline{R(P)}$ so that $H = M \oplus M^\perp$. 
Let $m \in M$. Then there exists a sequence $(m_i) \subset \R(P)$ such that $m_i \to m$. Then for any $n \in \N(P)$,
\begin{align*} 
(Pm|n)_H &= (P(\lim_{i \to \infty} m_i)|n)_H \\
            &= \lim_{i \to \infty} (Pm_i|n)_H \\
            &= \lim_{i \to \infty} (m_i|Pn)_H \\
            &= \lim_{i \to \infty} (m_i|0)_H \\
            &= 0
\end{align*} 
using the continuity of $P$ and the inner product, as well as the assumption that $P^* = P$. 
Therefore, $Pm \in \N(P)^\perp = M$. 
Next, let $m^\perp \in M^\perp = \overline{\R(P)}^\perp$. Then for all $x \in H$, 
\[(Pm^\perp|x)_H = (m^\perp|Pm)_H = 0\]
since $Pm \in M$, as shown above. 

Therefore, for any $x \in H$, we have shown that $x$ may be decomposed as $x = m + m^\perp$
and that \[Px = P(m + m^\perp) = Pm + Pm^\perp = m + 0 = m \in M\]
Therefore, $P$ is an orthogonal projection operator onto $M = \overline{\R(P)}$. 
\end{proof} 

The projection operator offers us new notation in which to express the Hilbert projection theorem (Luenberger 1969, p. 51). 
\begin{thm} \label{projection_theorem}
Let $M \subset H$ be a closed subspace and $x \in H$. Then the following statements are equivalent. 
\begin{itemize} 
\item $\norm{x - \hat{m}}_H \leq \norm{x - m}_H$ for all $m \in M$ 
\item $\hat{m} \in M$ and $x - \hat{m} \in M^\perp$
\item $\hat{m} = P_M x$
\end{itemize} 
\end{thm} 

\begin{proof} 
The first two statements are equivalent by the projection theorem, so it suffices to show that the last two 
statements imply each other. If the second statement holds, then $x = \hat{m} + (x - \hat{m})$ is the unique 
orthogonal decomposition for $x$ with respect to $M$. Thus, $P_M x = \hat{m}$. Now suppose the third 
statement holds. By definition of the projection operator $\hat{m} \in M$ and by proposition 
(\ref{projection_properties}),  $x - \hat{m} = (I - P_M)x \in M^\perp$. 
\end{proof} 

% Section 3: Formulating Linear Inverse Problems in Hilbert Space
\section{Formulating Linear Inverse Problems in Hilbert Space} \label{formulation}

 Recall from section \ref{inverse_problem_intro} that the natural approach to address the non-existence 
 and non-uniqueness of equation \eqref{inverse_problem} was to consider the least-squares problem 
 \begin{equation} 
 \inf_{x \in X} \norm{Fx - y}_Y \label{linear_least_squares}
 \end{equation} 
If we assume that $\mathcal{R}(F)$ is closed, 
then the existence of a unique $\hat{y} \in Y$ minimizing $\norm{y^\prime - y}_Y$ 
 over $\R(F)$ is guaranteed. However, we have not assumed that $F$ is injective and hence this does not 
 imply the uniqueness of a least-squares solution $\hat{x}$. Therefore, we must study the set of all 
 least-squares solutions, with an emphasis on the solution of minimal norm. 
 
 \begin{definition} 
(Generalized Solutions) 
 \begin{itemize}
 \item $\hat{x} \in X$ is called a least-squares solution of \eqref{linear_least_squares} if $\norm{F\hat{x} - y}_Y \leq \norm{Fx - y}_Y$ for all $x \in X$. $\mathbb{L}$ denotes the set of all such solutions. 
 \item $x^\dagger \in X$ is called a minimal norm least-squares solution if $x^\dagger \in \mathbb{L}$ and $\norm{x^\dagger}_X \leq \norm{\hat{x}}_X$ for all $\hat{x} \in \mathbb{L}$. 
 \end{itemize} 
\end{definition} 

\subsection{The Normal Equations}
We begin by proving a lemma that will be used frequently in the subsequent analysis. 

\begin{lemma} \label{range_lemma}
If $y = y^\prime + y^\perp \in \R(F) \oplus \R(F)^\perp$ then, 
\begin{itemize} 
\item $y^\perp \in \overline{\R(F)^\perp}$
\item $P_{\overline{\R(F)}} y = y^\prime \in \R(F)$. 
\end{itemize} 
\end{lemma} 

\begin{proof} 
By definition of the projection operator, $P_{\overline{\R(F)}} y \in \overline{\R(F)}$, but we seek to show that under the 
additional hypothesis $y \in \R(F) \oplus \R(F)^\perp$ that the projection is actually in $\R(F)$. We first note that 
$y^\prime \in \R(F) \subset \overline{\R(F)}$. If we show $y^\perp \in \overline{\R(F)}^\perp$ then the second result will immediately 
follow from the definition of the projection operator: 
\[P_{\overline{\R(F)}} y = P_{\overline{\R(F)}} (y^\prime + y^\perp) = y^\prime \in \R(F)\]
To this end, let $v \in \overline{\R(F)}$. Then there is a sequence $(y_i) \subset \R(F)$ such that $y_i \to v$. Using
the fact that $y^\perp \in \R(F)^\perp$ and the continuity of the inner product, we have 
\[0 = \lim_{i \to \infty} (y_i|y^\perp)_Y = (\lim_{i \to \infty} y_i|y^\perp)_Y = (v|y^\perp)_Y\]
which proves $y^\perp \in \overline{\R(F)}^\perp$. Thus, $P_{\overline{\R(F)}} y = y^\prime \in \R(F)$.
\end{proof} 

The following theorem generalizes the well-known normal equations to infinite-dimensional spaces. 

\begin{thm} \label{normal_equations}
The following are equivalent. 
\begin{itemize} 
\item $\hat{x} \in \mathbb{L}$
\item $F^* F\hat{x} = F^* y$
\item $F\hat{x} = P_{\overline{\R(F)}} y$
\end{itemize} 
\end{thm} 

\begin{proof} 
First note that $\inf_{x \in X} \norm{Fx - y}_Y = \inf_{y^\prime \in \R(F)} \norm{y^\prime - y}_Y$. By the projection theorem (Luenberger 1969, p. 51) 
we have \[\hat{y} = \inf_{y^\prime \in \R(F)} \norm{y^\prime - y}_Y \iff \hat{y} - y \perp \R(F)\] 
Therefore, $\hat{x} \in \mathbb{L}$ if and only if $F\hat{x} - y \in \R(F)^\perp$. We then obtain the decomposition
$y = F\hat{x} + (y - F\hat{x})$, with $F\hat{x} \in \R(F)$ and $y - F\hat{x} \in \R(F)^\perp$. Then by lemma (\ref{range_lemma}) it follows that
$P_{\overline{R(F)}} y = F\hat{x}$. This shows that the first statement implies the third. To show the converse, we apply theorem (\ref{projection_theorem})
to conclude \[\norm{F\hat{x} - y}_Y = \norm{P_{\overline{\R(F)}}y - y}_Y \leq \inf_{z \in \overline{\R(F)}} \norm{z - y}_Y \leq \inf_{z \in \R(F)} \norm{z - y}_Y = \inf_{x \in X} \norm{Fx - y}_Y\]
which shows that $\hat{x} \in \mathbb{L}$. To complete the proof, we show the equivalence of the first two statements. 
\begin{align*} 
\hat{x} = \inf_{x \in X} \norm{Fx - y}_Y &\iff (F\hat{x} - y)_Y \perp \R(F) \\
                                                            &\iff (F\hat{x} - y| Fx)_Y = 0 \ \forall x \in X \\
                                                            &\iff (F^*(F\hat{x} - y)| x)_X = 0 \ \forall x \in X \\
                                                            &\iff (F^*F\hat{x} - F^*y| x)_X = 0 \ \forall x \in X \\
                                                            &\iff F^*F\hat{x} - F^*y = 0 \\
                                                            &\iff F^*F\hat{x} = F^*y                                                            
\end{align*} 
where the first step uses the above portion of the proof and the others utilize the basic properties of the adjoint and inner product.
\end{proof} 

Interpreting \textit{normal} to mean \textit{orthogonal}, then the following result explains the name ``normal equations''; namely, that 
the residual vector is normal to $\R(F)$. While this follows directly from the projection theorem (Luenberger 1969, p. 51), we prove it
via the results developed above. 

\begin{corollary} \label{normal_residual}
Suppose $\hat{x} \in X$ satisfies $F^* F\hat{x} = F^* y$. Then $y - F\hat{x} \in \R(F)^\perp$.
\end{corollary}

\begin{proof} 
We have, \[y - F\hat{x} = y - P_{\overline{\R(F)}}y = (I - P_{\overline{\R(F)}})y = P_{\overline{\R(F)}^\perp}y\]
where the first equality follows from theorem \ref{normal_equations} and the third from proposition \ref{projection_properties}. 
It then follows from lemma \ref{range_lemma} that \[y - F\hat{x} = P_{\overline{\R(F)}^\perp}y \in \R(F)^\perp\]
\end{proof} 

\subsection{Characterization of Solutions}

\subsubsection{Assuming Closed Range}

Given our current assumptions, we cannot conclude the existence of a least-squares solution. The cleanest approach to deal with this 
issue is to assume $\R(F)$ is closed, allowing us to invoke the projection theorem (Luenberger 1969, p. 51) to conclude the existence of a solution. While this 
approach is straightforward, it is ultimately overly restrictive; indeed, there are important classes of linear maps that fail to have closed 
range. We therefore first characterize $\mathbb{L}$ under the assumption that $\R(F)$ is closed in order to build intuition, and then 
subsequently drop this assumption in favor of a more general theory. 
 
 \begin{prop}
 Suppose $\R(F)$ is closed. Then \eqref{linear_least_squares} has a unique minimal norm least-squares solution $x^\dagger$. Moreover, 
 $x^\dagger \in \N(F)^\perp$ and  $\mathbb{L} = x^\dagger + \N(F)$.
 \end{prop} 
 
\begin{proof} 
Since $\R(F)$ is closed, there exists a unique $\hat{y} \in Y$ satisfying $\hat{y} = \min_{y^\prime \in \R(F)} \norm{y^\prime - y}_{Y}$. Let $\hat{x} \in \mathbb{L}$ such that $F\hat{x} = \hat{y}$. Then, 
 \begin{align*} 
 \mathbb{L} &= \{x \in X: Fx = \hat{y}\} \\
                   &= \{x \in X: F(x - \hat{x}) = 0\} \\
                   &= \hat{x} + \{x - \hat{x} \in X: F(x - \hat{x}) = 0\} \\
                   &= \hat{x} + \N(F)
 \end{align*} 
 Therefore, $ \mathbb{L}$ is a translation of the closed subspace $\N(F)$. It then follows immediately that there exists a unique $x^\dagger \in \mathbb{L}$ of minimal norm, and moreover that $x^\dagger$ is orthogonal to $\N(F)$ (Luenberger 1969, p. 64). From the above derivation we clearly also have $\mathbb{L} = x^\dagger + \N(F)$
\end{proof} 

If either $X$ or $Y$ is finite-dimensional, then $\R(F)$ is closed and hence a solution always exists. 

\subsubsection{Dropping Assumption of Closed Range}

In the above section, we relied on the assumption that $\R(F)$ is closed to guarantee the existence of a least-squares solution
to \eqref{linear_least_squares} for every $y \in Y$. 
If $\R(F)$ is not closed, we cannot cannot assert existence for all $y$  
but we can restrict ourselves to consider the maximal subset of $Y$ that always admits a least-squares solution. 
The following results generalize those of the above section by discarding the assumption that $\R(F)$ is closed and instead 
restricting our attention to this maximal subset. 

\begin{thm} \label{existence_theorem}
$\mathbb{L}$ is non-empty (that is, \eqref{linear_least_squares} has a solution) if and only if $y \in \R(F) \oplus \R(F)^\perp$. 
\end{thm} 

\begin{proof} 
Let $\hat{x} \in \mathbb{L}$. Then $y = F\hat{x} + (y - F\hat{x}) \in \R(F) \oplus \R(F)^\perp$, following from corollary 
\ref{normal_residual}. Conversely, suppose $y = y^\prime + y^\perp \in \R(F) \oplus \R(F)^\perp$. Then by lemma 
\ref{range_lemma}, $P_{\overline{\R(F)}}y = y^\prime \in \R(F)$. There is thus an $\hat{x} \in X$ such that 
$F\hat{x} = y^\prime$. Since $P_{\overline{\R(F)}}y = F\hat{x}$, it then follows from theorem \ref{normal_equations} that 
$\hat{x} \in \mathbb{L}$. 
\end{proof} 

The above theorem provides a resolution to the existence question by giving a necessary and sufficient condition that 
a given problem has a least-squares solution. Combined with the next theorem, these results give a complete
characterization of $\mathbb{L}$. 

\begin{thm} \label{solution_characterization}
Suppose $y \in \R(F) \oplus \R(F)^\perp$. Then \eqref{linear_least_squares} has a unique minimal norm solution $x^\dagger \in \N(F)^\perp$. 
Moreover, $\mathbb{L} = x^\dagger + \N(F)$. 
\end{thm} 

\begin{proof} 
Since $y \in \R(F) \oplus \R(F)^\perp$, theorem \ref{existence_theorem} guarantees that $\mathbb{L}$ is non-empty. Let $x_1, x_2 \in \mathbb{L}$. 
Since $\N(F) \subset X$ is a closed subspace, we have $X = \N(F) \oplus \N(F)^\perp$. We thus have the unique decompositions
$x_1 = x_1^\prime + x_1^\perp \in \N(F) \oplus \N(F)^\perp$ and $x_2 = x_2^\prime + x_2^\perp \in \N(F) \oplus \N(F)^\perp$. Since 
$Fx_i^\perp = Fx_i - Fx_i^\prime = Fx_i$ then $x_1^\perp$, $x_2^\perp \in \mathbb{L}$. Then, 
\[F(x_1^\perp - x_2^\perp) = Fx_1^\perp - Fx_2^\perp = P_{\overline{\R(F)}}y - P_{\overline{\R(F)}}y = 0\]
where the second equality follows from theorem \ref{normal_equations}. Therefore, $x_1^\perp - x_2^\perp \in \N(F)$. 
But since $\N(F)^\perp$ is a subspace, it also follows that $x_1^\perp - x_2^\perp \in \N(F)^\perp$. Therefore, 
$x_1^\perp - x_2^\perp = 0$, or $x_1^\perp = x_2^\perp$. Letting $x^\dagger = x_1^\perp = x_2^\perp$, we have shown that
\[\hat{x} \in \mathbb{L} \implies \hat{x} \in x^\dagger + \N(F)\]
I claim that $x^\dagger$ is actually the minimal norm least squares solution. Indeed, for any $\hat{x} \in \mathbb{L}$ we have
shown that $\hat{x} = x^\dagger + x^\prime$, where $x^\prime \in \N(F)$. Then, 
\[\norm{\hat{x}}_X^2 = \norm{x^\dagger + x^\prime}_X^2 = \norm{x^\dagger}_X^2 + \norm{x^\prime}_X^2 \geq \norm{x^\dagger}_X^2\]
where the second equality follows from $x^\dagger \perp x^\prime$. This combined with the fact that $x^\dagger \in \mathbb{L}$ proves that
$x^\dagger$ is indeed a minimal norm least squares solution. Uniqueness follows from the fact that $\N(F)$ is closed (Luenberger 1969, p. 64). 

Since we know that $x^\dagger + \N(F) \subset \mathbb{L}$, it follows that $\mathbb{L} = x^\dagger + \N(F)$. 
\end{proof} 

\subsubsection{A First Look at Finding Solutions}
 
In the above section, we verified (under suitable assumptions) the existence of a unique minimal norm solution. 
We now turn our attention to developing techniques to find this solution. The standard tool in this effort is the 
\textit{Moore-Penrose pseudoinverse}. As we are not guaranteed a unique least-squares solution, 
the standard concept of an ``inverse'' does not apply. However, we now observe the utility of having 
proved results that guarantee a unique minimal norm solution, which allows us to generalize the 
inverse as an operator that maps to such solutions. 
 
 \begin{definition} 
 For $F \in B(X, Y)$, define $\tilde{F}$ to be the restriction of $F$ to $\N(F)^\perp$. The Moore-Penrose psuedoinverse $F^\dagger : \R(F) \oplus \R(F)^\perp \to X$ is then defined as follows: 
 \begin{itemize} 
 \item For all $y \in \R(F)$, $F^\dagger y := \tilde{F}^{-1}y$
 \item $F^\dagger$ is extended to $\R(F) \oplus \R(F)^\perp$ by requiring that $F^\dagger$ is linear and satisfies $F^\dagger y^\perp = 0$ for all $y^\perp \in \R(F)^\perp$
 \end{itemize} 
 \end{definition} 
 
Note that by definition $\tilde{F}$ has a trivial null space and we are therefore justified in talking 
about its inverse. Recall that a minimal norm solution is guaranteed for any $y \in \R(F) \oplus \R(F)^\perp$. 
This allows us to define $\R(F) \oplus \R(F)^\perp$ as the domain of $F^\dagger$. In particular, if $\R(F)$ is closed then $F^\dagger$ 
is defined on all of $Y$. The following result gives the true motivation for the above definition; namely, that the pseudoinverse maps 
to minimal norm solutions.

\begin{thm} 
Let $y \in \R(F) \oplus \R(F)^\perp$. Then $F^\dagger y$ is the minimal norm solution of \eqref{linear_least_squares}. 
\end{thm} 

\begin{proof} 
By theorem \ref{solution_characterization}, a unique minimal norm solution exists. By assumption we have the 
unique decomposition $y = \hat{y} + y^\perp \in \R(F) \oplus \R(F)^\perp$. Since $\hat{y} = y - y^\perp \in \R(F)^\perp$, 
it follows from the projection theorem (Luenberger 1969, p. 50) that \[\norm{\hat{y} - y}_Y \leq \norm{y^\prime - y}_Y\]
for all $y^\prime \in \R(F)$. By definition of $F^\dagger$, 
\[F^\dagger y = F^\dagger \hat{y} = \tilde{F}^{-1}\hat{y}\]
Applying $F$ to both sides, 
\[F(F^\dagger y) = \hat{y}\]
Thus $F^\dagger y \in \mathbb{L}$. By definition $F^\dagger y \in \N(F)^\perp$ so if follows from 
theorem \ref{solution_characterization} that $F^\dagger y$ is the minimal norm least squares solution. 
\end{proof} 

At this point, the pseudoinverse is a purely theoretical tool. Without suitable means to calculate $F^\dagger$, then 
the above definition does little to benefit actual applications. While there is a wide array of literature concerned with 
calculating $F^\dagger$ is various settings, this is beyond the scope of this introduction. 

  
\begin{thebibliography}{20}

\bibitem{Clason} C. Clason, \textit{Regularization of Inverse Problems: Lecture Notes}. 2021.

\bibitem{EL} M. J. Ehrhardt and L. F. Lang, \textit{Inverse Problems}. 2018.

\bibitem{Luenberger}  D. G. Luenberger, \textit{Optimization by Vector Space Methods}. Wiley, 1969. 

\bibitem{Royden} H. L. Royden and P. Fitzpatrick, \textit{Real Analysis}. Pearson, 2018.

\bibitem{Sullivan} T. J. Sullivan, \textit{Introduction to Uncertainty Quantification}. Springer, 2017.

\end{thebibliography}
 
\end{document} 