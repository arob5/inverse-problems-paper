\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem*{definition}{Definition}
\newtheorem*{prop}{Proposition}
\newtheorem*{lemma}{Lemma}

\begin{document}

\begin{center}
\Large
A Brief Introduction to Linear Inverse Problems in Hilbert Space
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Introduction
The study of inverse problems is generally concerned with approximating 
an unknown value using potentially corrupted or noisy data. The study of 
such problems finds many applications across the physical, social, and 
mathematical sciences. 

This document is meant to help you prepare an Article for
submission to \textsc{Mathematics Magazine}.  Of course,
editorial decisions depend entirely on
what you say and how you say it. Nonetheless, we will all save
time if you exercise some care in how you first present the paper.

Now that I have caught your attention with an interesting introductory paragraph,
here is what you will find:
specific information about the style of Articles in the \textsc{Magazine}
and a description of the \LaTeX\ code we prefer that you use
to prepare your manuscript.

Since this section is very clearly
an introduction, I thought that labeling it ``Introduction" would add
nothing.  Note that I am willing to use the first person in an Article
and you might be as well.  Another equally respectable choice is ``we,''
even when there is only one author; this can
create an author-reader partnership to
work through the mathematics together.
Whatever voice you choose, consistency is important.

% Section 1: Introduction to Inverse Problems
\section{Introduction to Inverse Problems} \label{inverse_problem_intro}

\subsection{Motivation}
Students of linear algebra may recall the prominence of the matrix equation
\begin{equation} 
Ax = b. \label{matrix_eqn}
\end{equation}  
Here we may consider $A$ as a linear map 
that acts on a given vector $x$ and produces the output vector $b$. 
This constitutes the ``forward problem'' in which we are given inputs and must 
calculate outputs. The corresponding ``inverse problem'' reverses the situation; we observe 
$b$ and must determine the input $x$ which produced it. In its simplest form we may assume 
$A$ is invertible, in which case the solution to the inverse problem is given by matrix 
inversion: $x = A^{-1}b$. This paper introduces a vast generalization of this inverse 
problem framework, shedding the simplifying assumptions of finite dimensionality
and invertibility. The motivation for such a general theory 
of inverse problems will become clear with examples, as the majority of interesting 
applications fail to enjoy the ``well-posedness'' of the above equation. 
To make these notions concrete, consider the following mapping:
\begin{eqnarray} 
&F: X \to Y \\
&F(x) = y \label{inverse_problem}
\end{eqnarray} 
This equation, which generalizes the above matrix equation, will constitute the primary subject 
of our analysis. The goal remains to compute $x$ given $y$. Note that I have been intentionally 
vague regarding specific properties of the map $F$ or spaces $X$ and $Y$. Indeed, varying the assumptions 
placed on these objects lead to drastically different avenues of analysis and conceptual interpretations
of inverse problems. 

\subsection{Ill-Posedness}

If all real-world examples of inverse problems admitted straightforward representations as in equation \eqref{matrix_eqn}
then the field of inverse problem theory would not exist. The essence of the field is in developing methods to 
deal with the so-called ``ill-posed'' cases. The following definition makes this notion concrete, 
and motivates much of the theory discussed in this paper. 

\begin{definition} 
(Hadamard's Conditions)The inverse problem given by equation \eqref{inverse_problem} is well-posed in the sense of Hadamard 
provided it satisfies the three following conditions:
\begin{enumerate} 
\item (Existence) A solution always exists; that is, given any $y \in Y$ there exists an $x \in X$ such that $F(x) = y$. 
\item (Uniqueness) The solution is unique; that is, if $F(x_1) = F(x_2)$ then $x_1 = x_2$.
\item (Stability) The solution is a continuous function of the input data; that is, if $F(x_n) \to y$  then $x_n \to x$
\end{enumerate} 
If any of these conditions are not met, the problem is called ill-posed. 
\end{definition} 

Given that the central task in studying inverse problems is to mitigate ill-posedness, it is worthwhile to make some comments here. 
Violations of the first two conditions are typically handled by 
instead seeking the ``closest'' solution in some sense; that is, by reformulating the problem as a minimum norm problem: 

\begin{equation}
\argmin_{x \in X} \norm{F(x) - y} \label{min_norm}
\end{equation}

 The violation of stability presents a more difficult challenge. In general we imagine $y$ to be data measured imprecisely. 
 Therefore, the concern is that small fluctuations in the observations $y$ may lead to large deviations in $x$. 
Our goal is to minimize the \textit{forward error} $\norm{x - x_0}$, where $x_0$ is some unknown true solution. However, 
given that we don't know $x_0$, we must instead focus our efforts on minimizing the \textit{backward error}, $\norm{F(x) - y}$. 
Stated in this terminology, the stability condition means that small backward errors imply small forward errors. 

 Regularization methods form the foundation of the toolkit used to address instability and will be discussed later in this paper. 

\subsection{Theoretical Framework}

Hilbert space provides a natural setting in which to study inverse problems. From a theoretical point of view, the structure provided by 
inner products leads to relatively simple characterizations of solutions, while practically, many real-world inverse problems may 
naturally be formulated in such settings. Therefore, throughout this paper, we consider the inverse problem \eqref{inverse_problem}
where $X$ and $Y$ are assumed to be real Hilbert spaces with respective inner products $(\cdot | \cdot)_X$ and $(\cdot | \cdot)_Y$. 
Moreover, we assume $F$ to be a bounded linear operator between these spaces. 

It is important to note that while we have restricted ourselves to these assumptions, inverse problems can be formulated in many 
other settings. Perhaps the most notable omission here is any sort of statistical treatment. It is common to treat $X$ and $Y$ as
probability spaces and $x$ and $y$ as random variables. In this setting, the given ``data'' is the conditional random variable 
$y|x$, and solving the inverse problem means estimating the random variable $x|y$. While such a formulation is common, this 
paper will treat the solution of \eqref{inverse_problem} as purely deterministic.  

 \subsection{Examples}
 
 An abstract, general formulation of inverse problems may not initially appear useful. 
 However, it soon becomes clear that the generality allows these techniques to be 
 applied to a wide array of problems across various fields. Many of these applications 
 are naturally formulated in infinite-dimensional vector spaces, and moreover suffer from
  ill-posedness. I provide three examples below of such applications. 
 
 \paragraph*{Statistical Estimation}
 Parameter estimation (or \textit{model calibration}), ubiquitous across the fields of statistics 
 and machine learning, may be formulated as an inverse problem. In this case we view 
 $F_\theta$ as a model or process that depends on some parameters $\theta$. We may
  alternatively view $\theta$ as an additional input to the system, in which case $F$ acts 
  on both the parameters and input data to produce the observed outputs; that is, 
  $F(\theta, x) = y$. The problem thus becomes: given observed outputs $y$ and inputs $x$, 
  estimate the parameters $\theta$ of the underlying process $F$. 
  
  \paragraph*{Differentiation}
 
 \textbf{TODO: Add two more examples}
 
 % Section 2: Preliminaries
 \section{Preliminaries: Operator Theory} \label{preliminaries}
 This paper assumes prior knowledge of Hilbert spaces, including the basic theory of bounded 
 linear maps between such spaces. However, the adjoint and projection operators, which are of central importance 
 to the study of inverse problems, are treated from scratch. As a notational note, $B(X, Y)$ denotes the space of all bounded
 (or equivalently, continuous) linear operators from $X$ to $Y$. For $F \in B(X, Y)$ we write $\norm{F}$
 to refer to the operator norm of $F$. $\mathcal{R}(F)$ and $\mathcal{N}(F)$ refer to the range and 
 null space of $F$, respectively.  
 
\subsection{Adjoints}
 \begin{definition} 
 (Adjoint) Let $F \in B(X, Y)$. Then the adjoint operator of $F$, $F^*: Y \to X$, is defined by 
 	\begin{equation*} 
	(Ax|y)_Y = (x|A^*y)_X
 	\end{equation*} 
	for all $x \in X$, $y \in Y$. 
 \end{definition} 
To justify this definition, consider the bounded linear functional $f_y(x) = (Ax|y)_Y$. By the Riesz representation theorem (Royden) 
there is a unique $x_y \in X$ satisfying $f_y(x) = (x|x_y)_X$ for all $x \in X$; that is, $(Ax|y)_Y = (x|x_y)_X$. 
We thus define $A^*y := x_y$. The adjoint helps to encode orthogonality relations between the spaces $X$ and $Y$. 
It may be interpreted as a generalization of the transpose operator for real-valued matrices; indeed, if $A \in \mathbb{R}^{n \times m}$, 
then $A^* = A^\prime$. The following three propositions outline the fundamental properties of adjoints that will leveraged throughout
this paper. 

\begin{prop} 
Let $F \in B(X, Y)$. Then $F^* \in B(Y, X)$ and $\norm{F^*} = \norm{F}$. 
\end{prop}

\begin{proof} 
The linearity of $F^*$ follows directly from the definition of the adjoint and the linearity of the inner product. 
\begin{align*} 
(x|F^*(\alpha_1 y_1 + \alpha_2 y_2))_X &= (Fx|\alpha_1 y_1 + \alpha_2 y_2)_Y \\
                                                               &= \alpha_1(Fx|y_1)_Y + \alpha_2(Fx|y_2)_Y \\
                                                               &= \alpha_1(x|F^*y_1)_X + \alpha_2(x|F^*y_2)_X \\
                                                               &= (x|\alpha_1 F^*y_1 + \alpha_2 F^*y_2)_X
\end{align*} 
To show that $F^*$ is bounded, let $y \in Y$ be non-zero and apply the Cauchy-Schwarz inequality to obtain,
\begin{align*} 
\norm{F^*y}^2_X = (F^*y|F^*y)_X = (F(F^*y)|y)_Y &\leq \norm{F(F^*y)}_Y\norm{y}_Y \\
                                                                               &\leq \norm{F}\norm{F^*y}_X\norm{y}_Y
\end{align*}
Thus, $\norm{F^*y}_X \leq \norm{F}\norm{y}_Y$ for all non-zero $y \in Y$ and therefore $\norm{F^*} \leq \norm{F}$. 
This verifies $F^* \in B(Y, X)$. By similar reasoning, we obtain the reverse inequality. 
\begin{align*} 
\norm{Fx}^2_Y = (Fx|Fx)_Y = (x|F^*(Fx))_X &\leq \norm{F^*(Fx)}_X\norm{x}_X \\
                                                                      &\leq \norm{F^*}\norm{Fx}_Y\norm{x}_X
\end{align*}
Therefore, $\norm{Fx} \leq \norm{F^*}\norm{x}_X$ for all non-zero $x \in X$ and so $\norm{F} \leq \norm{F^*}$. 
Thus, $\norm{F} = \norm{F^*}$
\end{proof} 

The first two results in the following proposition tell us that the adjoint is a linear map from $B(X, Y)$
to $B(Y, X)$.

\begin{prop} 
Let $F, G \in B(X, Y)$ and $\alpha \in \mathbb{R}$. Then,
\begin{itemize}
\item $(F + G)^* = F^* + G^*$
\item $(\alpha F)^* = \alpha F^*$
\item $F^{**} = F$
\end{itemize}  
\end{prop}

\begin{proof} 
The first result follows from definition of adjoint and linearity of the inner product. 
\begin{align*} 
(x|(F + G)^*y)_X &= ((F + G)x|y)_Y \\
                           &= (Fx|y)_Y + (Gx|y)_Y \\
                           &= (x|F^*y)_X + (x|G^*y)_X \\
                           &= (x|F^*y + G^*y)_X
\end{align*} 
Since $x$ and $y$ are arbitrary, this shows $(F + G)^* = F^* + G^*$. The second result follows from the 
same line of reasoning. For the third result, we know $F^{**}: X \to Y$ so 
\begin{equation*} 
(F^{**}x|y)_Y = (x|F^*y)_X = (Fx|y)_Y
\end{equation*} 
which shows $F^{**} = F$.
\end{proof} 

Recall from linear algebra that if $A \in \mathbb{R}^{n x m}$, then the row space of $A$ is orthogonal to 
the null space, and similarly the column space is orthogonal to the null space of $A^\prime$. The following
result generalizes this idea to Hilbert space, replacing the transpose with the adjoint. 

\begin{prop} 
Let $F \in B(X, Y)$. Then, 
\begin{itemize} 
\item $\mathcal{R}(F)^\perp = \mathcal{N}(F^*)$
\item $\overline{\mathcal{R}(F)} = \mathcal{N}(F^*)^\perp$
\item $\mathcal{R}(F^*)^\perp = \mathcal{N}(F)$
\item $\overline{\mathcal{R}(F^*)} = \mathcal{N}(F)^\perp$
\end{itemize} 
\end{prop} 

\begin{proof} 
Let $y^\prime \in \mathcal{N}(F^*)$ and $y \in \mathcal{R}(F)$. Then there exists $x \in X$
such that $Fx = y$. Then \[(Fx|y^\prime)_Y = (x|F^*y^\prime)_X = (x|0)_X = 0\]
so $\mathcal{N}(F^*) \subset \mathcal{R}(F)^\perp$. Now let $y^\perp \in \mathcal{R}(F)^\perp$. 
Then for any $x \in X$, \[(x|F^*y^\perp)_X = (Fx|y^\perp)_Y = 0\]
which implies $F^*y^\perp = 0$, or $y^\perp \in \mathcal{N}(F)$. 
Thus, $\mathcal{R}(F)^\perp = \mathcal{N}(F^*)$. Taking orthogonal complements, we obtain 
\[\mathcal{R}(F)^{\perp\perp} = \mathcal{N}(F^*)^\perp\]
or 
\[\overline{\mathcal{R}(F)} = \mathcal{N}(F^*)^\perp\]
The remaining two equalities follow directly from the fact that $F^{**} = F$. 
\end{proof} 


Note that if we assume the range of $F$ is closed then we may drop the closure in the second and third
results above. 

To conclude our introduction to the adjoint, we prove a result that more closely aligns with our goal of studying 
linear inverse problems. The restrictive assumption that the equation is consistent will be dropped in section
\textbf{section num}, where this result will be generalized to least-squares problems. 
\begin{prop} 
Let $F \in B(X, Y)$ with $\mathcal{R}(F) \subset Y$ closed. Assume there is at least one $x \in X$ satisfying
$Fx = y$. Then the vector $x^\dagger$ of minimum norm satisfying this equation is given by 
$x^\dagger = F^*y^\prime$, where $y^\prime$ is a solution of $FF^*y^\prime = y$. 
\end{prop} 

\begin{proof} 
Let $x \in X$ be any fixed solution to $Fx = y$. Then any vector of the form $x + k$, where $k \in \mathcal{N}(F)$, 
is also a solution. To show that this accounts for all possible solutions, let $x^\prime$ be another solution. 
This implies $F(x^\prime - x) = 0$ so $x^\prime - x \in \mathcal{N}(F)$ and thus $x^\prime$ must also be of the form 
$x + k$. Now that we have characterized the set of possible solutions, we must show how to select the solution
of minimal norm. The existence of such a unique solution follows from $\mathcal{N}(F)$ being a closed subspace, 
which implies the set of solutions to $Fx = y$ is a closed subspace. It then follows that the unique vector $x^\dagger$
of minimum norm is orthogonal to $\mathcal{N}(F)$ (Luenberger 1969, p. 64). From the previous proof and the assumption
that $F$ has closed range, \[x^\dagger \in \mathcal{N}(F)^\perp = \overline{\mathcal{R}(F^*)} = \mathcal{R}(F^*)\]
Therefore, there exists $y^\prime \in Y$ satisfying $F^*y^\prime = x^\dagger$. Since $x^\dagger$ is a solution of 
the original equation, we obtain $FF^*y^\prime = y$. 
\end{proof} 

\subsection{Projections}

\begin{definition} 
(Projection Operator) Let $M$ be a closed subspace of a Hilbert space $H$. Then each $x \in H$ has a unique
representation $x = m + m^\perp$, where $m \in M$ and $m \in M^\perp$. The operator $P_M: H \to H$ defined 
by $P_Mx = m$ is called the projection operator onto $M$. 
\end{definition}

\begin{prop} 
If $M \subset H$ is a closed subspace, then: 
\begin{itemize} 
\item $P_M \in B(H, H)$
\item If $M \neq \{0\}$, then $\norm{P_M} = 1$
\end{itemize} 
\end{prop}

\begin{proof} 
For linearity, let $x_1 = m_1 + m_1^\perp$, $x_2 = m_2 + m_2^\perp \in H$ and $\alpha \in \mathbb{R}$. Then 
$x_1 + x_2 = (m_1 + m_2) + (m_1^\perp + m_2^\perp)$, where the first term is in 
$M$ and the second in $M^\perp$. Thus, \[P_M (x_1 + x_2) = m_1 + m_1 = P_M x_1 + P_M x_2\]
$P_M (\alpha x_1) = \alpha P_M x_1$ follows by the same reasoning, so $P_M$ is linear. 

To show that $P_M$ is bounded, we apply the generalized Pythagorean Theorem to conclude: 
\[\norm{P_M x}_H^2 = \norm{m}_H^2 \leq \norm{m}_H^2 + \norm{m^\perp}_H^2 = \norm{x}_H^2\]
and therefore $\norm{P_M x}_H \leq \norm{x}_H$ for all $x \in H$ so $P_M$ is bounded. Thus, $P_M \in B(H, H)$
and $\norm{P_M} \leq 1$. 

For the reverse inequality, since $M \neq \{0\}$ we may select a vector $m \in M$ such that $\norm{m}_H = 1$.
Then $\norm{P_M m}_H = \norm{m}_H = 1$. This shows $\norm{P_M} \geq 1$. Therefore, $\norm{P_M} = 1$. 
\end{proof} 







% Section 3: Formulating Linear Inverse Problems in Hilbert Space
\section{Formulating Linear Inverse Problems in Hilbert Space} \label{formulation}

 Recall from section \ref{inverse_problem_intro} that the natural approach to address the non-existence 
 and non-uniqueness of equation \eqref{inverse_problem} was to consider the least-squares problem 
 \begin{equation} 
 \inf_{x \in X} \norm{Fx - y}_Y \label{linear_least_squares}
 \end{equation} 
If we assume that $\mathcal{R}(F)$ is closed, 
then the existence of a unique $\hat{y} \in Y$ minimizing $\norm{y^\prime - y}_Y$ 
 over $\mathcal{F}$ is guaranteed. However, we have not assumed that $F$ is injective and hence this does not 
 imply the uniqueness of a least-squares solution $\hat{x}$. Therefore, we must study the set of all 
 least-squares solutions, with an emphasis on the solution of minimal norm. 

\subsection{Characterization of Solutions}

\begin{definition} 
(Generalized Solutions) 
 \begin{itemize}
 \item $\hat{x} \in X$ is called a least-squares solution of \eqref{linear_least_squares} if $\norm{F\hat{x} - y}_Y \leq \norm{Fx - y}_Y$ for all $x \in X$. $\mathbb{L}$ denotes the set of all such solutions. 
 \item $x^\dagger \in X$ is called a minimal norm least-squares solution if $x^\dagger \in \mathbb{L}$ and $\norm{x^\dagger}_X \leq \norm{\hat{x}}_X$ for all $\hat{x} \in \mathbb{L}$. 
 \end{itemize} 
\end{definition} 

\subsubsection{Assuming Closed Range}

 Note that in general $\mathbb{L}$ may be empty. The following result relies on the assumption that $\R(F)$ is closed to ensure the existence of a solution. 
 
 \begin{prop}
 Suppose $\R(F)$ is closed. Then \eqref{linear_least_squares} has a unique minimal norm least-squares solution $x^\dagger$. Moreover, 
 $x^\dagger \in \N(F)^\perp$ and  $\mathbb{L} = x^\dagger + \N(F)$.
 \end{prop} 
 
\begin{proof} 
Since $\R(F)$ is closed, there exists a unique $\hat{y} \in Y$ satisfying $\hat{y} = \min_{y^\prime \in \R(F)} \norm{y^\prime - y}_{Y}$. Let $\hat{x} \in \mathbb{L}$ such that $F\hat{x} = \hat{y}$ (Luenberger 1969, p. 64). Then, 
 \begin{align*} 
 \mathbb{L} &= \{x \in X: Fx = \hat{y}\} \\
                   &= \{x \in X: F(x - \hat{x}) = 0\} \\
                   &= \hat{x} + \{x - \hat{x} \in X: F(x - \hat{x}) = 0\} \\
                   &= \hat{x} + \N(F)
 \end{align*} 
 Therefore, $ \mathbb{L}$ is a translation of the closed subspace $\N(F)$. It then follows immediately that there exists a unique $x^\dagger \in \mathbb{L}$ of minimal norm, and moreover that $x^\dagger$ is orthogonal to $\N(F)$ (Luenberger 1969, p. 64). From the above derivation we clearly also have $\mathbb{L} = x^\dagger + \N(F)$
\end{proof} 

If either $X$ or $Y$ is finite-dimensional, then $\R(F)$ is closed and hence solutions always exist. However, there are important applications defined in 
infinite-dimensional spaces where $\R(F)$ is not guaranteed to be closed. In these cases, we will have to concern ourselves
with the fact that a solution may not exist. 

\subsubsection{Not Assuming Closed Range}

In the above results, we rely on the assumption that $\R(F)$ is closed to guarantee the existence of least-squares solutions. 
However, there are many examples in infinite-dimensional spaces where this assumption does not hold. If $\R(F)$ is not closed, 
we cannot guarantee the existence of a solution to \eqref{linear_least_squares} for every $y \in Y$, but we can restrict ourselves to 
consider the maximal subset of $Y$ that always admits a least-squares solution. The following results generalize the above 
results by discarding the assumption that $\R(F)$ is closed and instead restricting our attention to this maximal subset. 

\begin{lemma} 
If $y \in \R(F) \oplus \R(F)^\perp$ then $P_{\overline{\R(F)}} y \in \R(F)$. 
\end{lemma} 

\begin{proof} 
By definition of the projection operator, $P_{\overline{\R(F)}} y \in \overline{\R(F)}$, but we seek to show that under the 
additional hypothesis $y \in \R(F) \oplus \R(F)^\perp$ that the projection is actually in $\R(F)$. We first note that 
by assumption $y$ may be decomposed as $y = y^\prime + y^\perp$, where $y^\prime \in \R(F) \subset \overline{\R(F)}$ and 
$y^\perp \in \R(F)^\perp$. Note that if we show $y^\perp \in \overline{\R(F)}^\perp$ then from the definition of the projection 
operator we would have \[P_{\overline{\R(F)}} y = P_{\overline{\R(F)}} (y^\prime + y^\perp) = y^\prime \in \R(F)\]
as desired. To this end, let $v \in \overline{\R(F)}$. Then there is a sequence $(y_i) \subset \R(F)$ such that $y_i \to v$. Using
the fact that $y^\perp \in \R(F)^\perp$ and the continuity of the inner product, we have 
\[0 = \lim_{i \to \infty} (y_i|y^\perp) = (\lim_{i \to \infty} y_i|y^\perp) = (v|y^\perp)\]
which proves $y^\perp \in \overline{\R(F)}^\perp$. Thus, $P_{\overline{\R(F)}} y = y^\prime \in \R(F)$.
\end{proof} 

With this useful fact established, we may fully characterize $\mathbb{L}$. 

\begin{prop} 
$\mathbb{L}$ is non-empty (that is, \eqref{linear_least_squares} has a solution) if and only if $y \in \R(F) \oplus \R(F)^\perp$. 
\end{prop} 

\subsubsection{The Normal Equations}

The following theorem, which provides another useful characterization of least-squares solutions, 
 generalizes the well-known normal equations to infinite-dimensional spaces. 

\begin{prop} 
$\hat{x} \in \mathbb{L}$ if and only if $F^* F\hat{x} = F^* y$. In this case $F\hat{x} - y \in \R(F)^\perp$.
\end{prop} 

\begin{proof} 
First note that $\inf_{x \in X} \norm{Tx - y}_Y = \inf_{y^\prime \in \R(F)} \norm{y^\prime - y}_Y$. By the projection theorem (Luenberger 3.3 Theorem 1) we have \[\hat{y} = \inf_{y^\prime \in \R(F)} \norm{y^\prime - y}_Y \iff \hat{y} - y \perp \R(F)\] 
The orthogonality condition follows from the fact that $\hat{x} \in \mathbb{L}$ if and only if $F\hat{x} = \hat{y}$. 
To obtain the normal equations, consider: 
\begin{align*} 
\hat{x} = \inf_{x \in X} \norm{Fx - y}_X &\iff (F\hat{x} - y) \perp \R(F) \\
                                                            &\iff (F\hat{x} - y| Fx) = 0 \ \forall x \in X \\
                                                            &\iff (F^*(F\hat{x} - y)| x) = 0 \ \forall x \in X \\
                                                            &\iff (F^*F\hat{x} - F^*y| x) = 0 \ \forall x \in X \\
                                                            &\iff F^*F\hat{x} - F^*y = 0 \\
                                                            &\iff F^*F\hat{x} = F^*y                                                            
\end{align*} 
where the first step uses the above portion of the proof and the others utilize the basic properties of the adjoint and inner product.
\end{proof} 

 \subsection{Finding Minimal Norm Solutions}
 
In the above section, we verified (under suitable assumptions) the existence of a unique minimal norm solution. 
We now turn our attention to developing techniques to find this solution. The standard tool in this effort is the 
\textit{Moore-Penrose pseudoinverse}. As we are not guaranteed a unique least-squares solution, 
the standard concept of an ``inverse'' does not apply. However, we now observe the utility of having 
proved results that guarantee a unique minimal norm solution, which allows us to generalize the 
inverse as an operator that maps to such solutions. 
 
 \begin{definition} 
 For $F \in B(X, Y)$, define $\tilde{F}$ to be the restriction of $F$ to $\N(F)^\perp$. The Moore-Penrose psuedoinverse $F^\dagger : \R(F) \oplus \R(F)^\perp \to Y$ is then defined as follows: 
 \begin{itemize} 
 \item For all $y \in \R(F)$, $F^\dagger y := F^{-1}y$
 \item $F^\dagger$ is extended to $\R(F) \oplus \R(F)^\perp$ by requiring that $F^\dagger$ is linear and satisfies $F^\dagger y^\perp = 0$ for all $y^\perp \in \R(F)^\perp$
 \end{itemize} 
 \end{definition} 
 
Note that by definition $\tilde{F}$ has a trivial null space and we are therefore justified in talking 
about its inverse. Recall that a minimal norm solution is guaranteed for any $y \in Y$ when $\R(F)$ is closed. 
Instead of making this restrictive assumption in defining the pseudoinverse, we instead choose to define 
 $F^\dagger$ on a smaller domain $y \in \R(F) \oplus \R(F)^\perp$ 
guaranteed uniqueness of the minimal norm solution. It is therefore unsurprising that we define the 
domain of $F^\dagger$ to be $\R(F) \oplus \R(F)^\perp$. In particular, if $\R(F)$ is closed then $F^\dagger$ 
is defined on all of $Y$. While the above definition may seem cryptic, the next result shows that it addresses
 our problem of finding minimal norm solutions.

\textbf{Proposition}: Let $y \in \R(F) \oplus \R(F)^\perp$. Then $T^\dagger y$ is the minimal norm solution of the least-squares problem. 

\smallskip 

\textit{Proof}: By \textbf{prop num}, a unique minimal norm solution exists. There are unique $\hat{y} \in \R(F)$ and $y^\perp \in \R(F)^\perp$ such that $y = \hat{y} + y^\perp$. Since $y - \hat{y} \perp \R(F)$ it follows from the projection theorem (Luenberger 3.3 Theorem 1) that $\hat{y} = \argmin_{y^\prime \in \R(F)} \norm{y^\prime - y}_Y$. By definition of $T^\dagger$, we have \[T^\dagger y = T^\dagger \hat{y} = \tilde{T}^{-1}\hat{y}\]
 Applying $T$ to both sides, \[T(T^\dagger y) = \hat{y}\]
 Thus $T^\dagger y \in \mathbb{L}$. By definition $T^\dagger y \in (\Ker T)^\perp$ so $T^\dagger y$ is the minimal norm solution (\textbf{prop num}). $\qquad \blacksquare$
 
 
\begin{thebibliography}{20}

\bibitem{Luenberger}  Luenberger, David G., \textit{Optimization by Vector Space Methods}. Wiley, 1969. 

\end{thebibliography}
 
\end{document} 